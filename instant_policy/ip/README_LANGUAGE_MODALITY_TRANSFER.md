# Language Modality Transfer for Instant Policy

## Zero-Shot Robot Control via Natural Language Instructions

This document provides a comprehensive technical overview of the language modality transfer implementation for the Instant Policy framework, enabling robots to execute tasks from natural language descriptions instead of point cloud demonstrations.

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Background: The Instant Policy Bottleneck](#2-background-the-instant-policy-bottleneck)
3. [Architecture Overview](#3-architecture-overview)
4. [Implementation Details](#4-implementation-details)
5. [Training Pipeline](#5-training-pipeline)
6. [Inference Pipeline](#6-inference-pipeline)
7. [Key Design Decisions](#7-key-design-decisions)
8. [File Reference](#8-file-reference)
9. [Usage Guide](#9-usage-guide)
10. [Results & Evaluation](#10-results--evaluation)

---

## Complete Architecture Diagram

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e1f5fe', 'primaryTextColor': '#01579b', 'primaryBorderColor': '#0288d1', 'lineColor': '#0288d1', 'secondaryColor': '#fff3e0', 'tertiaryColor': '#e8f5e9'}}}%%

flowchart TB
    subgraph TITLE[" "]
        direction LR
        T1["<b>LANGUAGE MODALITY TRANSFER FOR INSTANT POLICY</b><br/>Zero-Shot Robot Control via Natural Language"]
    end

    subgraph TRAINING["<b>ğŸ“ TRAINING PHASE</b>"]
        direction TB

        subgraph TEACHER["<b>TEACHER (Frozen)</b>"]
            direction TB
            DEMOS["ğŸ“¦ Point Cloud Demos<br/><i>2 demos Ã— 10 timesteps</i><br/>[2, 10, 16, 1024]"]
            CURR_OBS_T["ğŸ“· Current Observation<br/><i>Point cloud + gripper pose</i>"]

            SCENE_ENC_T["<b>Scene Encoder</b><br/><i>PointNet++</i><br/>Point cloud â†’ 16 nodes Ã— 1024D"]
            LOCAL_ENC["<b>Local Encoder Ïƒ</b><br/><i>Graph Transformer</i><br/>2 layers"]
            COND_ENC["<b>Conditional Encoder Ï†</b><br/><i>Graph Transformer</i><br/>Aggregates demo info"]

            DEMO_BOTTLENECK["â­ <b>DEMO BOTTLENECK</b><br/>[B, 6, 1024]<br/><i>6 gripper nodes Ã— 1024D</i><br/><b>= 6,144 dimensions</b>"]

            DEMOS --> SCENE_ENC_T
            CURR_OBS_T --> SCENE_ENC_T
            SCENE_ENC_T --> LOCAL_ENC
            LOCAL_ENC --> COND_ENC
            COND_ENC --> DEMO_BOTTLENECK
        end

        subgraph STUDENT["<b>STUDENT (Trainable)</b>"]
            direction TB
            LANG_INPUT["ğŸ’¬ Language Input<br/><i>'Press the button.'</i>"]
            CURR_OBS_S["ğŸ“· Current Observation<br/><i>Point cloud + gripper pose</i>"]

            SBERT["<b>Sentence-BERT</b><br/><i>all-mpnet-base-v2</i><br/>~110M params (frozen)<br/>Text â†’ [B, 768]"]

            SCENE_ENC_S["<b>Scene Encoder</b><br/>(shared, frozen)"]
            LOCAL_ENC_S["<b>Local Encoder Ïƒ</b><br/>(shared, frozen)"]

            subgraph LANG_ENC["<b>Language Encoder Î¸</b><br/><i>~130M trainable params</i>"]
                direction TB
                LANG_PROJ["<b>Language Projection</b><br/>Linear(768 â†’ 1024)<br/>787K params"]

                subgraph GRAPH["<b>Heterogeneous Graph</b>"]
                    direction LR
                    SCENE_NODES["ğŸ”µ Scene<br/>16 nodes"]
                    GRIPPER_NODES["ğŸŸ¢ Gripper<br/>6 nodes"]
                    LANG_NODE["ğŸŸ¡ Language<br/>1 node"]
                end

                GRAPH_TRANS["<b>Graph Transformer</b><br/>4 layers Ã— 16 heads<br/>6 edge types Ã— 3 node types<br/>~129M params"]
            end

            LANG_BOTTLENECK["â­ <b>LANG BOTTLENECK</b><br/>[B, 6, 1024]<br/><i>6 gripper nodes Ã— 1024D</i>"]

            LANG_INPUT --> SBERT
            CURR_OBS_S --> SCENE_ENC_S
            SCENE_ENC_S --> LOCAL_ENC_S
            SBERT --> LANG_PROJ
            LOCAL_ENC_S --> GRAPH
            LANG_PROJ --> LANG_NODE
            GRAPH --> GRAPH_TRANS
            GRAPH_TRANS --> LANG_BOTTLENECK
        end

        subgraph LOSS["<b>ğŸ“‰ LOSS FUNCTION</b>"]
            direction TB
            INFONCE["<b>InfoNCE Loss</b><br/>temperature = 0.07<br/>weight = 1.0<br/><i>Discriminative: different tasks<br/>â†’ different bottlenecks</i>"]
            L2["<b>L2 Loss (MSE)</b><br/>weight = 0.1<br/><i>Generative: exact<br/>bottleneck alignment</i>"]
            TOTAL_LOSS["<b>Total Loss</b><br/>L = 1.0Ã—InfoNCE + 0.1Ã—L2"]

            INFONCE --> TOTAL_LOSS
            L2 --> TOTAL_LOSS
        end

        DEMO_BOTTLENECK -.->|"Target"| LOSS
        LANG_BOTTLENECK -.->|"Prediction"| LOSS
    end

    subgraph INFERENCE["<b>ğŸš€ INFERENCE PHASE</b>"]
        direction TB

        LANG_INF["ğŸ’¬ 'Press the button.'"]
        OBS_INF["ğŸ“· Current Observation"]

        SBERT_INF["<b>Sentence-BERT</b><br/>[768D]"]
        ENCODER_INF["<b>Trained Language Encoder Î¸</b>"]

        BOTTLENECK_INF["â­ <b>BOTTLENECK</b><br/>[1, 6, 1024]"]

        subgraph FROZEN_DECODER["<b>FROZEN ACTION DECODER</b>"]
            direction TB
            DUMMY["ğŸ“¦ Dummy Demos<br/><i>All zeros, correct structure</i><br/><i>Maintains graph topology</i>"]
            COND_INF["<b>cond_encoder</b><br/><i>Updates scene features</i>"]
            INJECT["ğŸ’‰ <b>INJECT</b><br/><i>Overwrite gripper nodes</i>"]
            ACTION_ENC["<b>Action Encoder Ïˆ</b><br/><i>Graph Transformer</i>"]
            HEADS["<b>Prediction Heads</b><br/>Translation (3D)<br/>Rotation (3D)<br/>Gripper (1D)"]
            DIFFUSION["<b>Diffusion Denoising</b><br/>4-8 iterations"]

            DUMMY --> COND_INF
            COND_INF --> INJECT
            INJECT --> ACTION_ENC
            ACTION_ENC --> HEADS
            HEADS --> DIFFUSION
        end

        ACTIONS["ğŸ¤– <b>Robot Actions</b><br/>[1, 8, 4, 4] SE(3)"]

        LANG_INF --> SBERT_INF
        OBS_INF --> ENCODER_INF
        SBERT_INF --> ENCODER_INF
        ENCODER_INF --> BOTTLENECK_INF
        BOTTLENECK_INF --> INJECT
        DIFFUSION --> ACTIONS
    end

    subgraph DIMENSIONS["<b>ğŸ“ TENSOR DIMENSIONS</b>"]
        direction LR
        subgraph DIM_INPUT["Inputs"]
            D1["scene_x: [B, 16, 1024]"]
            D2["gripper_x: [B, 6, 1024]"]
            D3["lang_emb: [B, 768]"]
        end
        subgraph DIM_GRAPH["Graph per sample"]
            D4["Nodes: 23 total<br/>(16 scene + 6 gripper + 1 lang)"]
            D5["Edges: 411 total<br/>(256 + 96 + 36 + 16 + 6 + 1)"]
            D6["Edge attr: 126D<br/>(63D Ã— 2 positional)"]
        end
        subgraph DIM_OUTPUT["Output"]
            D7["bottleneck: [B, 6, 1024]<br/>= 6,144 dimensions"]
        end
    end

    subgraph PARAMS["<b>ğŸ“Š PARAMETERS & MEMORY</b>"]
        direction LR
        subgraph PARAM_COUNT["Parameter Counts"]
            P1["Language Encoder: ~130M"]
            P2["Sentence-BERT: ~110M (frozen)"]
            P3["Teacher Model: ~150-200M (frozen)"]
        end
        subgraph MEMORY["Memory Usage"]
            M1["Training: ~3-4 GB GPU"]
            M2["Inference: ~1.8 GB GPU"]
        end
    end

    subgraph RESULTS["<b>ğŸ“ˆ TRAINING RESULTS (50K steps)</b>"]
        direction LR
        R1["Similarity: 0.0 â†’ <b>0.952</b>"]
        R2["Loss: 4.0 â†’ <b>0.100</b>"]
        R3["L2 Loss: 12.0 â†’ <b>0.188</b>"]
        R4["Contrastive: 2.5 â†’ <b>0.081</b>"]
    end

    subgraph EDGE_TYPES["<b>ğŸ”— EDGE TYPES IN LANGUAGE ENCODER</b>"]
        direction TB
        E1["(scene, rel, scene): 16Ã—16 = 256 edges"]
        E2["(scene, rel, gripper): 16Ã—6 = 96 edges"]
        E3["(gripper, rel, gripper): 6Ã—6 = 36 edges"]
        E4["(language, lang_to_scene): 1Ã—16 = 16 edges"]
        E5["(language, lang_to_gripper): 1Ã—6 = 6 edges"]
        E6["(language, lang_self): 1Ã—1 = 1 edge"]
    end

    %% Styling
    style TITLE fill:#1565c0,stroke:#0d47a1,color:#fff
    style TRAINING fill:#e3f2fd,stroke:#1976d2
    style INFERENCE fill:#fff3e0,stroke:#ff9800
    style TEACHER fill:#e8f5e9,stroke:#4caf50
    style STUDENT fill:#fce4ec,stroke:#e91e63
    style LOSS fill:#fff9c4,stroke:#fbc02d
    style FROZEN_DECODER fill:#f3e5f5,stroke:#9c27b0
    style DEMO_BOTTLENECK fill:#ffeb3b,stroke:#f57f17,color:#000
    style LANG_BOTTLENECK fill:#ffeb3b,stroke:#f57f17,color:#000
    style BOTTLENECK_INF fill:#ffeb3b,stroke:#f57f17,color:#000
    style LANG_ENC fill:#e1bee7,stroke:#7b1fa2
    style DIMENSIONS fill:#e0f7fa,stroke:#00838f
    style PARAMS fill:#fafafa,stroke:#616161
    style RESULTS fill:#c8e6c9,stroke:#2e7d32
    style EDGE_TYPES fill:#ffe0b2,stroke:#ef6c00
```

---

## 1. Executive Summary

### The Problem
Instant Policy requires 1-2 point cloud demonstrations (3D scene + gripper poses) to execute new tasks. This is limiting because:
- Demonstrations must be recorded for each new task
- Real-time task specification is not possible
- Human-robot interaction requires demonstration interfaces

### The Solution
We implement **language modality transfer** that:
- Replaces point cloud demonstrations with natural language instructions
- Reuses the frozen pre-trained action decoder (Ïˆ)
- Requires only a small language-annotated dataset for training
- Enables zero-shot transfer to language-defined tasks

### Key Insight
The Instant Policy architecture has a natural **information bottleneck** where all task-relevant information from demonstrations is compressed into 6 gripper nodes Ã— 1024 dimensions. We learn to approximate this bottleneck from language + current observation, then reuse the frozen action decoder.

```
DEMO PATH:    demos â†’ Ïƒ â†’ Ï• â†’ [BOTTLENECK] â†’ Ïˆ â†’ actions
LANGUAGE PATH: obs + lang â†’ Î¸ â†’ [BOTTLENECK] â†’ Ïˆ â†’ actions (frozen)
```

---

## 2. Background: The Instant Policy Bottleneck

### Original Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INSTANT POLICY ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  DEMONSTRATIONS              CURRENT OBSERVATION                            â”‚
â”‚  (2 demos Ã— 10 timesteps)    (point cloud + gripper pose)                   â”‚
â”‚         â”‚                              â”‚                                    â”‚
â”‚         â–¼                              â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚              SCENE ENCODER (PointNet++)                  â”‚                â”‚
â”‚  â”‚         Extract 16 scene nodes with 1024D features       â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                              â”‚                                    â”‚
â”‚         â–¼                              â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚              LOCAL ENCODER Ïƒ (Graph Transformer)         â”‚                â”‚
â”‚  â”‚    Process scene + gripper nodes with spatial edges      â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                        â”‚                                                    â”‚
â”‚                        â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚           CONDITIONAL ENCODER Ï• (Graph Transformer)      â”‚                â”‚
â”‚  â”‚      Aggregate demo information to current gripper       â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                        â”‚                                                    â”‚
â”‚                        â–¼                                                    â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                â”‚
â”‚  â•‘            â˜… BOTTLENECK: 6 gripper nodes Ã— 1024D â˜…       â•‘                â”‚
â”‚  â•‘       Contains ALL task-relevant information from demos  â•‘                â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                â”‚
â”‚                        â”‚                                                    â”‚
â”‚                        â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚            ACTION ENCODER Ïˆ (Graph Transformer)          â”‚                â”‚
â”‚  â”‚         Generate future action predictions               â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                        â”‚                                                    â”‚
â”‚                        â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚              PREDICTION HEADS (3 MLPs)                   â”‚                â”‚
â”‚  â”‚         Translation (3D) + Rotation (3D) + Gripper (1D)  â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                        â”‚                                                    â”‚
â”‚                        â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚              DIFFUSION DENOISING (4-8 steps)             â”‚                â”‚
â”‚  â”‚         Iteratively refine noisy actions                 â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Bottleneck

After the conditional encoder (Ï•), the **current gripper nodes** contain:
- **6 nodes** representing gripper geometry (center, tail, sides, fingers)
- **1024 dimensions per node** encoding task context
- **Total**: 6,144 dimensional task representation

This bottleneck implicitly encodes:
- What object to manipulate
- Where the target location is
- What trajectory to follow
- How to grip the object

**Key Observation**: The action encoder (Ïˆ) only needs this bottleneck to generate correct actions. It doesn't care HOW the bottleneck was computedâ€”from demos or from language.

---

## 3. Architecture Overview

### Language Modality Transfer Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LANGUAGE MODALITY TRANSFER ARCHITECTURE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  TRAINING PHASE                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•                                                              â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    TEACHER (Frozen)    â”‚    â”‚         STUDENT (Trainable)           â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                        â”‚    â”‚                                        â”‚   â”‚
â”‚  â”‚  demos + current obs   â”‚    â”‚  current obs + language embedding     â”‚   â”‚
â”‚  â”‚         â”‚              â”‚    â”‚         â”‚              â”‚               â”‚   â”‚
â”‚  â”‚         â–¼              â”‚    â”‚         â–¼              â–¼               â”‚   â”‚
â”‚  â”‚    scene_encoder       â”‚    â”‚    scene_encoder   Sentence-BERT      â”‚   â”‚
â”‚  â”‚         â”‚              â”‚    â”‚    (frozen)        (pre-trained)       â”‚   â”‚
â”‚  â”‚         â–¼              â”‚    â”‚         â”‚              â”‚               â”‚   â”‚
â”‚  â”‚    local_encoder Ïƒ     â”‚    â”‚    local_encoder      â”‚               â”‚   â”‚
â”‚  â”‚         â”‚              â”‚    â”‚    (frozen)           â”‚               â”‚   â”‚
â”‚  â”‚         â–¼              â”‚    â”‚         â”‚              â”‚               â”‚   â”‚
â”‚  â”‚    cond_encoder Ï•      â”‚    â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚
â”‚  â”‚         â”‚              â”‚    â”‚                â”‚                       â”‚   â”‚
â”‚  â”‚         â–¼              â”‚    â”‚                â–¼                       â”‚   â”‚
â”‚  â”‚    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•—      â”‚    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚    â•‘ BOTTLENECK â•‘      â”‚    â”‚    â”‚  LANGUAGE ENCODER Î¸     â”‚        â”‚   â”‚
â”‚  â”‚    â•‘ [6, 1024]  â•‘â—„â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â–ºâ”‚  (Graph Transformer)    â”‚        â”‚   â”‚
â”‚  â”‚    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•      â”‚    â”‚    â”‚  4 layers, 16 heads     â”‚        â”‚   â”‚
â”‚  â”‚         â–²              â”‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â”‚         â”‚              â”‚    â”‚                â”‚                       â”‚   â”‚
â”‚  â”‚    TARGET for          â”‚    â”‚                â–¼                       â”‚   â”‚
â”‚  â”‚    contrastive +       â”‚    â”‚         â•”â•â•â•â•â•â•â•â•â•â•â•â•â•—                â”‚   â”‚
â”‚  â”‚    L2 loss             â”‚    â”‚         â•‘ BOTTLENECK â•‘                â”‚   â”‚
â”‚  â”‚                        â”‚    â”‚         â•‘ [6, 1024]  â•‘                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•                â”‚   â”‚
â”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  LOSS FUNCTION                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•                                                              â”‚
â”‚  L = 1.0 Ã— InfoNCE(lang_bottleneck, demo_bottleneck) +                      â”‚
â”‚      0.1 Ã— MSE(lang_bottleneck, demo_bottleneck)                            â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  INFERENCE PHASE                                                            â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                            â”‚
â”‚                                                                             â”‚
â”‚  "Press the button" â”€â”€â–º Sentence-BERT â”€â”€â–º [768D embedding]                  â”‚
â”‚                                                   â”‚                         â”‚
â”‚  current observation â”€â”€â–º scene_encoder â”€â”€â–º local_encoder                    â”‚
â”‚                                                   â”‚                         â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                          â”‚         LANGUAGE ENCODER Î¸                 â”‚     â”‚
â”‚                          â”‚    (scene_x, gripper_x, lang_emb) â†’ Î¸     â”‚     â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                   â”‚                         â”‚
â”‚                                                   â–¼                         â”‚
â”‚                                          â•”â•â•â•â•â•â•â•â•â•â•â•â•â•—                     â”‚
â”‚                                          â•‘ BOTTLENECK â•‘                     â”‚
â”‚                                          â•‘ [6, 1024]  â•‘                     â”‚
â”‚                                          â•šâ•â•â•â•â•â•¤â•â•â•â•â•â•â•                     â”‚
â”‚                                                â”‚                            â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                     â”‚              FROZEN ACTION DECODER                  â”‚ â”‚
â”‚                     â”‚  cond_encoder â†’ [inject bottleneck] â†’ action_encoderâ”‚ â”‚
â”‚                     â”‚              â†’ heads â†’ diffusion                    â”‚ â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                â”‚                            â”‚
â”‚                                                â–¼                            â”‚
â”‚                                        ROBOT ACTIONS                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Implementation Details

### 4.1 Language Encoder Architecture

**File**: `models/language_encoder.py`

The `LanguageConditionedEncoder` is a heterogeneous graph transformer that fuses current observation features with language semantics.

#### Node Types
| Node Type | Count | Dimensions | Description |
|-----------|-------|------------|-------------|
| `scene` | 16 | 1024D | Scene point features from PointNet++ |
| `gripper` | 6 | 1024D | Gripper node features |
| `language` | 1 | 1024D | Projected language embedding |

#### Edge Types
| Edge Type | Description | Edge Features |
|-----------|-------------|---------------|
| `(scene, rel, scene)` | Dense scene-to-scene | 126D positional encoding |
| `(scene, rel, gripper)` | Dense scene-to-gripper | 126D positional encoding |
| `(gripper, rel, gripper)` | Dense gripper-to-gripper | 126D positional encoding |
| `(language, lang_to_scene, scene)` | Language broadcasts to all scene nodes | 126D learned embedding |
| `(language, lang_to_gripper, gripper)` | Language broadcasts to all gripper nodes | 126D learned embedding |
| `(language, lang_self, language)` | Self-loop (PyG requirement) | 126D learned embedding |

#### Architecture Details
```python
LanguageConditionedEncoder:
    - lang_proj: Linear(768 â†’ 1024)           # Project SBERT to hidden dim
    - pos_embd: PositionalEncoder(3D â†’ 63D)   # Sinusoidal position encoding
    - lang_edge_emb: Parameter([1, 126])      # Learned language edge features
    - transformer: GraphTransformer(
          in_channels=1024,
          hidden_channels=1024,
          heads=16,                            # 1024/64 = 16 heads
          num_layers=4,
          edge_dim=126,
          dropout=0.0,
          norm='layer'
      )
```

#### Forward Pass
```python
def forward(scene_x, scene_pos, gripper_x, gripper_pos, lang_emb):
    # 1. Project language embedding: [B, 768] â†’ [B, 1024]
    lang_emb = self.lang_proj(lang_emb)

    # 2. Build heterogeneous graph with all node/edge types
    graph = self._build_language_graph(...)

    # 3. Run graph transformer (4 layers of message passing)
    x_dict = self.transformer(graph.x_dict, graph.edge_index_dict, graph.edge_attr_dict)

    # 4. Return only gripper features as bottleneck: [B, 6, 1024]
    return x_dict['gripper'].view(batch_size, 6, -1)
```

#### Tensor Dimensions Throughout Forward Pass

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TENSOR DIMENSIONS (B = batch size)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  INPUTS                                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€                                                                     â”‚
â”‚  scene_x:      [B, 16, 1024]    â† 16 scene nodes Ã— 1024D features           â”‚
â”‚  scene_pos:    [B, 16, 3]       â† 16 scene nodes Ã— 3D positions             â”‚
â”‚  gripper_x:    [B, 6, 1024]     â† 6 gripper nodes Ã— 1024D features          â”‚
â”‚  gripper_pos:  [B, 6, 3]        â† 6 gripper nodes Ã— 3D positions            â”‚
â”‚  lang_emb:     [B, 768]         â† Sentence-BERT embedding                   â”‚
â”‚                                                                             â”‚
â”‚  LANGUAGE PROJECTION                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚  lang_proj(lang_emb):                                                       â”‚
â”‚    [B, 768] â†’ Linear(768, 1024) â†’ [B, 1024]                                 â”‚
â”‚                                                                             â”‚
â”‚  GRAPH CONSTRUCTION (per batch element)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                         â”‚
â”‚  Nodes:                                                                     â”‚
â”‚    scene:     16 nodes Ã— 1024D  = 16,384 values                             â”‚
â”‚    gripper:   6 nodes Ã— 1024D   = 6,144 values                              â”‚
â”‚    language:  1 node Ã— 1024D    = 1,024 values                              â”‚
â”‚    TOTAL:     23 nodes Ã— 1024D  = 23,552 values per sample                  â”‚
â”‚                                                                             â”‚
â”‚  Edges (dense connectivity):                                                â”‚
â”‚    (scene, rel, scene):           16 Ã— 16 = 256 edges                       â”‚
â”‚    (scene, rel, gripper):         16 Ã— 6  = 96 edges                        â”‚
â”‚    (gripper, rel, gripper):       6 Ã— 6   = 36 edges                        â”‚
â”‚    (language, lang_to_scene):     1 Ã— 16  = 16 edges                        â”‚
â”‚    (language, lang_to_gripper):   1 Ã— 6   = 6 edges                         â”‚
â”‚    (language, lang_self):         1 Ã— 1   = 1 edge                          â”‚
â”‚    TOTAL:                         411 edges per sample                      â”‚
â”‚                                                                             â”‚
â”‚  Edge Attributes:                                                           â”‚
â”‚    Positional edges: 126D (63D src encoding + 63D dst encoding)             â”‚
â”‚    Language edges:   126D (learned parameter, shared across all)            â”‚
â”‚                                                                             â”‚
â”‚  POSITIONAL ENCODING                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚  Input:  3D position (x, y, z)                                              â”‚
â”‚  Frequencies: 10 (log-spaced)                                               â”‚
â”‚  Output: 3 + 3Ã—10Ã—2 = 63D  (original + sin/cos for each freq Ã— each dim)   â”‚
â”‚  Edge attr: 63D Ã— 2 = 126D (concatenate src and dst encodings)              â”‚
â”‚                                                                             â”‚
â”‚  GRAPH TRANSFORMER (4 layers)                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚  Per layer (GraphTransformerBlock):                                         â”‚
â”‚    Input:  x_dict with node features [N_type, 1024]                         â”‚
â”‚    TransformerConv:                                                         â”‚
â”‚      - heads: 16                                                            â”‚
â”‚      - head_dim: 1024 / 16 = 64                                             â”‚
â”‚      - Q, K, V projections per edge type                                    â”‚
â”‚    MLP: [1024 â†’ 1024 â†’ 1024] with GELU + LayerNorm                          â”‚
â”‚    Output: x_dict with updated features [N_type, 1024]                      â”‚
â”‚                                                                             â”‚
â”‚  OUTPUT                                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€                                                                     â”‚
â”‚  gripper_out: [B, 6, 1024]      â† 6 gripper nodes Ã— 1024D = bottleneck      â”‚
â”‚                                                                             â”‚
â”‚  BOTTLENECK SIZE                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚
â”‚  6 Ã— 1024 = 6,144 dimensions per sample                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Parameter Count Breakdown

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PARAMETER COUNT (Language Encoder Only)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. LANGUAGE PROJECTION                                                     â”‚
â”‚     lang_proj: Linear(768 â†’ 1024)                                           â”‚
â”‚       Weight: 768 Ã— 1024 = 786,432                                          â”‚
â”‚       Bias:   1024                                                          â”‚
â”‚       Subtotal: 787,456 parameters                                          â”‚
â”‚                                                                             â”‚
â”‚  2. LANGUAGE EDGE EMBEDDING                                                 â”‚
â”‚     lang_edge_emb: Parameter([1, 126])                                      â”‚
â”‚       Subtotal: 126 parameters                                              â”‚
â”‚                                                                             â”‚
â”‚  3. GRAPH TRANSFORMER (4 layers Ã— 6 edge types)                             â”‚
â”‚                                                                             â”‚
â”‚     Per GraphTransformerBlock (homogeneous, before to_hetero):              â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
â”‚     TransformerConv(1024, 64, heads=16, edge_dim=126):                      â”‚
â”‚       lin_key:   1024 Ã— 1024 = 1,048,576                                    â”‚
â”‚       lin_query: 1024 Ã— 1024 = 1,048,576                                    â”‚
â”‚       lin_value: 1024 Ã— 1024 = 1,048,576                                    â”‚
â”‚       lin_edge:  126 Ã— 1024  = 129,024                                      â”‚
â”‚       lin_skip:  1024 Ã— 1024 = 1,048,576  (root_weight=True)                â”‚
â”‚       bias:      1024                                                       â”‚
â”‚       TransformerConv subtotal: 4,323,352                                   â”‚
â”‚                                                                             â”‚
â”‚     MLP([1024, 1024, 1024]):                                                â”‚
â”‚       Linear1: 1024 Ã— 1024 + 1024 = 1,049,600                               â”‚
â”‚       Linear2: 1024 Ã— 1024 + 1024 = 1,049,600                               â”‚
â”‚       LayerNorm: 1024 Ã— 2 = 2,048                                           â”‚
â”‚       MLP subtotal: 2,101,248                                               â”‚
â”‚                                                                             â”‚
â”‚     Per block total: 6,424,600                                              â”‚
â”‚                                                                             â”‚
â”‚     After to_hetero (Ã—6 edge types, Ã—3 node types):                         â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚     TransformerConv creates separate weights per edge type:                 â”‚
â”‚       6 edge types Ã— (lin_key + lin_query + lin_value + lin_edge + skip)    â”‚
â”‚       = 6 Ã— 4,323,352 â‰ˆ 25,940,112 per layer                                â”‚
â”‚                                                                             â”‚
â”‚     MLP creates separate weights per node type:                             â”‚
â”‚       3 node types Ã— 2,101,248 = 6,303,744 per layer                        â”‚
â”‚                                                                             â”‚
â”‚     Per hetero layer: ~32,243,856                                           â”‚
â”‚     Ã— 4 layers = ~128,975,424                                               â”‚
â”‚                                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚  TOTAL LANGUAGE ENCODER: ~130 million parameters                            â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚                                                                             â”‚
â”‚  For comparison:                                                            â”‚
â”‚  - Sentence-BERT (all-mpnet-base-v2): ~110M parameters (frozen)             â”‚
â”‚  - Teacher model (Instant Policy): ~150-200M parameters (frozen)            â”‚
â”‚  - Language encoder (trainable): ~130M parameters                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Memory Footprint

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MEMORY USAGE ESTIMATES                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  TRAINING (batch_size=16)                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Language encoder parameters:  130M Ã— 4 bytes = ~520 MB                     â”‚
â”‚  Gradients:                    130M Ã— 4 bytes = ~520 MB                     â”‚
â”‚  Optimizer states (AdamW):     130M Ã— 8 bytes = ~1.04 GB                    â”‚
â”‚  Activations (estimated):      ~500 MB - 1 GB                               â”‚
â”‚  Teacher model (frozen):       ~800 MB (no gradients)                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  Total training:               ~3-4 GB GPU memory                           â”‚
â”‚                                                                             â”‚
â”‚  INFERENCE (batch_size=1)                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚  Language encoder:             ~520 MB                                      â”‚
â”‚  Teacher model:                ~800 MB                                      â”‚
â”‚  Sentence-BERT:                ~440 MB                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  Total inference:              ~1.8 GB GPU memory                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Sentence-BERT Integration

**File**: `utils/language_utils.py`

We use pre-trained Sentence-BERT (`all-mpnet-base-v2`) to encode natural language instructions into fixed 768-dimensional vectors.

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-mpnet-base-v2')  # Downloads ~420MB on first use
embedding = model.encode("Press the button")       # â†’ [768] tensor
```

**Why Sentence-BERT?**
- Semantic similarity: "Press the button" â‰ˆ "Push the button down"
- Fixed-length output: Any text â†’ 768D vector
- Pre-trained on billions of sentence pairs
- No fine-tuning needed

### 4.3 Language Templates

**File**: `utils/language_utils.py`

Each of the 17 RLBench tasks has 3 natural language templates:

```python
TASK_TEMPLATES = {
    'push_button': [
        'Press the button.',
        'Push the button down.',
        'Activate the button.',
    ],
    'phone_on_base': [
        'Place the phone on its base.',
        'Put the phone onto the base station.',
        'Set the phone down on the charging base.',
    ],
    # ... 15 more tasks
}
```

**Purpose**:
- Provide language annotations for training data
- Enable paraphrase robustness testing
- Support data augmentation (random template selection)

### 4.4 Training Configuration

**File**: `configs/language_config.py`

```python
config = {
    'lang_emb_dim': 768,              # Sentence-BERT output dimension
    'lang_num_layers': 4,             # Graph transformer depth
    'lang_lr': 1e-4,                  # Learning rate
    'lang_weight_decay': 1e-2,        # AdamW weight decay
    'contrastive_temperature': 0.07,  # InfoNCE temperature
    'contrastive_weight': 1.0,        # InfoNCE loss weight
    'l2_weight': 0.1,                 # MSE loss weight
}
```

---

## 5. Training Pipeline

### 5.1 Data Preparation

**File**: `scripts/build_language_dataset.py`

Before training, each demonstration file (`data_*.pt`) must be annotated with language embeddings:

```bash
python scripts/build_language_dataset.py \
    --data_dir ./data/train/push_button \
    --task_name push_button \
    --device cuda \
    --add_text
```

**What it does**:
1. Load each `data_*.pt` file
2. Sample a random language template for the task
3. Encode with Sentence-BERT â†’ 768D vector
4. Add `lang_emb` (and optionally `lang_text`) to the file
5. Save modified file

**Before**:
```python
data = {
    'demos': [...],      # 2 demonstration trajectories
    'live': [...],       # Current observation
    'actions': [...],    # Ground truth actions
}
```

**After**:
```python
data = {
    'demos': [...],
    'live': [...],
    'actions': [...],
    'lang_emb': tensor([768]),      # â† ADDED
    'lang_text': "Press the button." # â† ADDED (optional)
}
```

### 5.2 Training Loop

**File**: `train_language.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           TRAINING LOOP                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  for each batch:                                                            â”‚
â”‚                                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚ STEP 1: Extract Teacher Bottleneck (frozen, no gradients)          â”‚  â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚    â”‚  with torch.no_grad():                                              â”‚  â”‚
â”‚    â”‚      demo_bottleneck = teacher.get_demo_bottleneck(data)            â”‚  â”‚
â”‚    â”‚      # Shape: [B, 6, 1024]                                          â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚ STEP 2: Extract Current Node Features (frozen)                     â”‚  â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚    â”‚  with torch.no_grad():                                              â”‚  â”‚
â”‚    â”‚      x_dict = teacher.local_encoder(...)                            â”‚  â”‚
â”‚    â”‚      current_scene_x = x_dict['scene'][current_mask]                â”‚  â”‚
â”‚    â”‚      current_gripper_x = x_dict['gripper'][current_mask]            â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚ STEP 3: Generate Language Bottleneck (trainable)                   â”‚  â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚    â”‚  lang_bottleneck = lang_encoder(                                    â”‚  â”‚
â”‚    â”‚      current_scene_x,                                               â”‚  â”‚
â”‚    â”‚      current_scene_pos,                                             â”‚  â”‚
â”‚    â”‚      current_gripper_x,                                             â”‚  â”‚
â”‚    â”‚      current_gripper_pos,                                           â”‚  â”‚
â”‚    â”‚      data.lang_emb        # [B, 768]                                â”‚  â”‚
â”‚    â”‚  )                                                                  â”‚  â”‚
â”‚    â”‚  # Shape: [B, 6, 1024]                                              â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚ STEP 4: Compute Loss                                               â”‚  â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚    â”‚                                                                     â”‚  â”‚
â”‚    â”‚  # Flatten bottlenecks: [B, 6, 1024] â†’ [B, 6144]                    â”‚  â”‚
â”‚    â”‚  lang_flat = normalize(lang_bottleneck.reshape(B, -1))              â”‚  â”‚
â”‚    â”‚  demo_flat = normalize(demo_bottleneck.reshape(B, -1))              â”‚  â”‚
â”‚    â”‚                                                                     â”‚  â”‚
â”‚    â”‚  # InfoNCE Contrastive Loss                                        â”‚  â”‚
â”‚    â”‚  logits = lang_flat @ demo_flat.T / 0.07   # [B, B] similarity     â”‚  â”‚
â”‚    â”‚  labels = [0, 1, 2, ..., B-1]              # Diagonal matching     â”‚  â”‚
â”‚    â”‚  contrastive = cross_entropy(logits, labels)                       â”‚  â”‚
â”‚    â”‚                                                                     â”‚  â”‚
â”‚    â”‚  # L2 Regression Loss                                              â”‚  â”‚
â”‚    â”‚  l2_loss = MSE(lang_bottleneck, demo_bottleneck)                   â”‚  â”‚
â”‚    â”‚                                                                     â”‚  â”‚
â”‚    â”‚  # Combined Loss                                                   â”‚  â”‚
â”‚    â”‚  loss = 1.0 * contrastive + 0.1 * l2_loss                          â”‚  â”‚
â”‚    â”‚                                                                     â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚ STEP 5: Backprop & Update (only lang_encoder)                      â”‚  â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚    â”‚  optimizer.zero_grad()                                              â”‚  â”‚
â”‚    â”‚  loss.backward()           # Only flows through lang_encoder        â”‚  â”‚
â”‚    â”‚  optimizer.step()                                                   â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 Loss Functions

#### InfoNCE Contrastive Loss

**File**: `train_language.py` (lines 14-22)

```python
def info_nce_loss(lang_bottleneck, demo_bottleneck, temperature):
    batch_size = lang_bottleneck.shape[0]
    # 1. Flatten and normalize to unit vectors
    lang_flat = F.normalize(lang_bottleneck.reshape(batch_size, -1), dim=-1)
    demo_flat = F.normalize(demo_bottleneck.reshape(batch_size, -1), dim=-1)

    # 2. Compute similarity matrix [B, B]
    logits = torch.matmul(lang_flat, demo_flat.t()) / temperature

    # 3. Labels: diagonal entries are positive pairs
    labels = torch.arange(batch_size, device=logits.device)

    # 4. Symmetric cross-entropy loss
    loss = (F.cross_entropy(logits, labels) + F.cross_entropy(logits.t(), labels)) / 2
    return loss
```

**Detailed Explanation:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      InfoNCE LOSS - STEP BY STEP                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  SETUP: Batch of 4 samples with different tasks                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                            â”‚
â”‚                                                                             â”‚
â”‚    Sample 0: "Press the button"     â†’ lang_bottleneck[0], demo_bottleneck[0]â”‚
â”‚    Sample 1: "Pick up the phone"    â†’ lang_bottleneck[1], demo_bottleneck[1]â”‚
â”‚    Sample 2: "Close the box"        â†’ lang_bottleneck[2], demo_bottleneck[2]â”‚
â”‚    Sample 3: "Slide the block"      â†’ lang_bottleneck[3], demo_bottleneck[3]â”‚
â”‚                                                                             â”‚
â”‚  STEP 1: Flatten and Normalize                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                              â”‚
â”‚                                                                             â”‚
â”‚    lang_bottleneck: [4, 6, 1024] â†’ reshape â†’ [4, 6144] â†’ normalize â†’ [4, 6144]â”‚
â”‚    demo_bottleneck: [4, 6, 1024] â†’ reshape â†’ [4, 6144] â†’ normalize â†’ [4, 6144]â”‚
â”‚                                                                             â”‚
â”‚    Normalization: Each row becomes a unit vector (L2 norm = 1)              â”‚
â”‚    This makes cosine similarity = dot product                               â”‚
â”‚                                                                             â”‚
â”‚  STEP 2: Compute Similarity Matrix                                          â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                          â”‚
â”‚                                                                             â”‚
â”‚    logits = lang_flat @ demo_flat.T / temperature                           â”‚
â”‚           = [4, 6144] @ [6144, 4] / 0.07                                    â”‚
â”‚           = [4, 4] similarity matrix                                        â”‚
â”‚                                                                             â”‚
â”‚                        demo_0   demo_1   demo_2   demo_3                    â”‚
â”‚                       (button) (phone)   (box)   (block)                    â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚    lang_0 (button)   â”‚  HIGH  â”‚  low   â”‚  low   â”‚  low   â”‚  â† row 0        â”‚
â”‚                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚    lang_1 (phone)    â”‚  low   â”‚  HIGH  â”‚  low   â”‚  low   â”‚  â† row 1        â”‚
â”‚                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚    lang_2 (box)      â”‚  low   â”‚  low   â”‚  HIGH  â”‚  low   â”‚  â† row 2        â”‚
â”‚                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚    lang_3 (block)    â”‚  low   â”‚  low   â”‚  low   â”‚  HIGH  â”‚  â† row 3        â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                             â”‚
â”‚    DIAGONAL = positive pairs (same task)                                    â”‚
â”‚    OFF-DIAGONAL = negative pairs (different tasks)                          â”‚
â”‚                                                                             â”‚
â”‚  STEP 3: Temperature Scaling (Ï„ = 0.07)                                     â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                    â”‚
â”‚                                                                             â”‚
â”‚    Temperature controls the "sharpness" of the distribution:                â”‚
â”‚                                                                             â”‚
â”‚    Raw similarity:     sim = 0.8 (before temp)                              â”‚
â”‚    After temp:         sim / 0.07 = 11.4 (much larger!)                     â”‚
â”‚                                                                             â”‚
â”‚    Low temperature (0.07):                                                  â”‚
â”‚      â†’ Makes softmax very "peaky"                                           â”‚
â”‚      â†’ Model strongly penalized for ANY confusion between tasks             â”‚
â”‚      â†’ Forces clear separation in bottleneck space                          â”‚
â”‚                                                                             â”‚
â”‚    High temperature (1.0):                                                  â”‚
â”‚      â†’ Softmax more uniform                                                 â”‚
â”‚      â†’ Weaker gradients, slower learning                                    â”‚
â”‚                                                                             â”‚
â”‚  STEP 4: Cross-Entropy Loss (Row-wise)                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                      â”‚
â”‚                                                                             â”‚
â”‚    labels = [0, 1, 2, 3]  â† Each sample should match its own demo           â”‚
â”‚                                                                             â”‚
â”‚    For row 0: softmax([HIGH, low, low, low]) should predict class 0         â”‚
â”‚    For row 1: softmax([low, HIGH, low, low]) should predict class 1         â”‚
â”‚    ...                                                                      â”‚
â”‚                                                                             â”‚
â”‚    loss_lang_to_demo = CrossEntropy(logits, labels)                         â”‚
â”‚                      = -log(softmax(logits)[i, i]) averaged over i          â”‚
â”‚                                                                             â”‚
â”‚    Interpretation: "Given a language bottleneck, find the matching demo"    â”‚
â”‚                                                                             â”‚
â”‚  STEP 5: Cross-Entropy Loss (Column-wise / Symmetric)                       â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                      â”‚
â”‚                                                                             â”‚
â”‚    loss_demo_to_lang = CrossEntropy(logits.T, labels)                       â”‚
â”‚                                                                             â”‚
â”‚    Interpretation: "Given a demo bottleneck, find the matching language"    â”‚
â”‚                                                                             â”‚
â”‚    Why symmetric?                                                           â”‚
â”‚      â†’ Ensures both directions are learned equally                          â”‚
â”‚      â†’ Language â†’ Demo: "This instruction means this bottleneck"            â”‚
â”‚      â†’ Demo â†’ Language: "This bottleneck corresponds to this instruction"   â”‚
â”‚                                                                             â”‚
â”‚  FINAL LOSS                                                                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•                                                                 â”‚
â”‚                                                                             â”‚
â”‚    loss = (loss_lang_to_demo + loss_demo_to_lang) / 2                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why InfoNCE Works for This Task:**

| Property | Explanation |
|----------|-------------|
| **Discriminative** | Forces "press button" â‰  "pick phone" in bottleneck space |
| **Batch-efficient** | Uses all BÂ² pairs in a batch (B positives, BÂ²-B negatives) |
| **Temperature** | Ï„=0.07 creates strong gradients for learning fine distinctions |
| **Symmetric** | Learns both languageâ†’demo and demoâ†’language mappings |

**Numerical Example:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NUMERICAL EXAMPLE (B=4, Ï„=0.07)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Raw cosine similarities (before temperature):                              â”‚
â”‚                                                                             â”‚
â”‚         demo_0  demo_1  demo_2  demo_3                                      â”‚
â”‚  lang_0 [ 0.95   0.10   0.15   0.08 ]   â† good alignment                    â”‚
â”‚  lang_1 [ 0.12   0.92   0.20   0.11 ]                                       â”‚
â”‚  lang_2 [ 0.18   0.15   0.88   0.14 ]                                       â”‚
â”‚  lang_3 [ 0.09   0.13   0.11   0.91 ]                                       â”‚
â”‚                                                                             â”‚
â”‚  After dividing by temperature (0.07):                                      â”‚
â”‚                                                                             â”‚
â”‚         demo_0  demo_1  demo_2  demo_3                                      â”‚
â”‚  lang_0 [13.57   1.43   2.14   1.14 ]   â† 0.95/0.07 = 13.57                 â”‚
â”‚  lang_1 [ 1.71  13.14   2.86   1.57 ]                                       â”‚
â”‚  lang_2 [ 2.57   2.14  12.57   2.00 ]                                       â”‚
â”‚  lang_3 [ 1.29   1.86   1.57  13.00 ]                                       â”‚
â”‚                                                                             â”‚
â”‚  Softmax of row 0: [0.987, 0.005, 0.008, 0.004]                             â”‚
â”‚                     â†‘ almost 1.0 at correct position                        â”‚
â”‚                                                                             â”‚
â”‚  Cross-entropy for row 0: -log(0.987) = 0.013  â† very low loss!             â”‚
â”‚                                                                             â”‚
â”‚  If alignment was poor (sim=0.5 at diagonal):                               â”‚
â”‚  After temp: 0.5/0.07 = 7.14                                                â”‚
â”‚  Softmax: [0.85, 0.05, 0.05, 0.05]                                          â”‚
â”‚  Cross-entropy: -log(0.85) = 0.16  â† higher loss, stronger gradient         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Hyperparameters:**

| Parameter | Value | Effect |
|-----------|-------|--------|
| `temperature` | 0.07 | Low = sharp distributions, strong task separation |
| `contrastive_weight` | 1.0 | Full weight on InfoNCE loss |
| `batch_size` | 16 | 16 positives + 240 negatives per batch |

**Purpose**: Ensure different tasks have different bottlenecks (discriminative learning)

#### L2 Regression Loss

```python
l2_loss = F.mse_loss(lang_bottleneck, demo_bottleneck)
```

**Purpose**: Direct bottleneck alignment for faster convergence

### 5.4 Training Metrics

| Metric | Start | Target | Meaning |
|--------|-------|--------|---------|
| `loss` | ~6.0 | ~1.5-2.0 | Combined loss |
| `contrastive` | ~6.0 | ~1.5 | InfoNCE loss |
| `l2_loss` | ~1.0 | ~0.1-0.2 | MSE loss |
| `sim` | ~0.0-0.2 | ~0.8-0.9 | Cosine similarity between bottlenecks |

---

## 6. Inference Pipeline

### 6.1 Control Loop

**File**: `eval_language.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          INFERENCE CONTROL LOOP                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  INPUT: Language instruction "Press the button."                            â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ SETUP (once)                                                          â”‚  â”‚
â”‚  â”‚   lang_emb = SentenceBERT.encode("Press the button.")  # [768]        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ CONTROL LOOP (repeat ~30 times until task complete)                   â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚   â”‚ 1. OBSERVE                                                      â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    point_cloud = get_observation()                              â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    gripper_pose = get_gripper_pose()                            â”‚ â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                              â”‚                                        â”‚  â”‚
â”‚  â”‚                              â–¼                                        â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚   â”‚ 2. COMPUTE BOTTLENECK (once per control step)                   â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    scene_x, gripper_x = teacher.local_encoder(point_cloud)      â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    bottleneck = lang_encoder(scene_x, gripper_x, lang_emb)      â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    # Shape: [1, 6, 1024]                                        â”‚ â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                              â”‚                                        â”‚  â”‚
â”‚  â”‚                              â–¼                                        â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚   â”‚ 3. DIFFUSION DENOISING (4-8 iterations, reuse bottleneck)       â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    noisy_actions = random_init()                                â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    for k in [7, 6, 5, 4, 3, 2, 1, 0]:                           â”‚ â”‚  â”‚
â”‚  â”‚   â”‚        preds = teacher.forward_from_bottleneck(data, bottleneck)â”‚ â”‚  â”‚
â”‚  â”‚   â”‚        noisy_actions = denoise_step(noisy_actions, preds, k)    â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    actions = noisy_actions  # [1, 8, 4, 4] SE(3) transforms     â”‚ â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                              â”‚                                        â”‚  â”‚
â”‚  â”‚                              â–¼                                        â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚   â”‚ 4. EXECUTE                                                      â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    robot.execute(actions[0])  # Execute first action            â”‚ â”‚  â”‚
â”‚  â”‚   â”‚    if task_complete: break                                      â”‚ â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  OUTPUT: Task completed (button pressed)                                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Bottleneck Injection

**Critical Implementation Detail**

The `forward_from_bottleneck` method in `model.py`:

```python
def forward_from_bottleneck(self, data, bottleneck):
    # 1. Run local encoder (frozen)
    x_dict = self.local_encoder(...)

    # 2. Run cond_encoder (frozen) - IMPORTANT!
    #    This updates scene features that action_encoder needs
    x_dict = self.cond_encoder(...)

    # 3. INJECT language bottleneck into current gripper nodes
    current_mask = self._get_current_gripper_mask()
    x_dict['gripper'][current_mask] = bottleneck.view(-1, 1024)

    # 4. Run action encoder (frozen)
    x_dict = self.action_encoder(...)

    # 5. Generate predictions
    preds = self.prediction_heads(x_dict['gripper'])
    return preds
```

**Why run `cond_encoder` before injection?**
- The `action_encoder` was trained on features processed by `cond_encoder`
- `cond_encoder` also updates **scene features** via cross-attention
- Skipping it causes distribution mismatch â†’ collapsed action magnitudes

### 6.3 Paraphrase Evaluation

**File**: `eval_language.py` with `--paraphrase_file`

Test robustness to language variations:

```bash
python eval_language.py \
    --task_name push_button \
    --paraphrase_file paraphrases.txt \
    --num_rollouts 5
```

Where `paraphrases.txt`:
```
Press the button.
Push the button down.
Hit the red button.
Tap the button gently.
Activate the switch.
```

**Output**:
```
sr=0.800 sim=1.000 text="Press the button."
sr=0.600 sim=0.923 text="Push the button down."
sr=0.600 sim=0.891 text="Hit the red button."
sr=0.400 sim=0.845 text="Tap the button gently."
sr=0.200 sim=0.712 text="Activate the switch."
Paraphrase SR mean=0.520 std=0.179
```

**Why paraphrases work**: Sentence-BERT maps semantically similar text to nearby regions in embedding space. The language encoder learns a smooth mapping, so nearby inputs â†’ similar bottlenecks â†’ similar actions.

---

## 7. Key Design Decisions

### 7.1 Why Bottleneck Transfer?

| Alternative | Drawback |
|-------------|----------|
| Fine-tune entire model | Catastrophic forgetting, expensive |
| Train language-to-action end-to-end | Need huge language-action datasets |
| Use VLM directly | Too slow for real-time control |
| **Bottleneck transfer** âœ“ | Modular, efficient, small dataset |

### 7.2 Why Frozen Teacher?

- **Preserves demo capability**: Original demo-based inference still works
- **Stable training**: Only small language encoder updates
- **Fast training**: ~8-12 hours instead of days
- **No architectural changes**: Same graph structure, edge types

### 7.3 Why InfoNCE + L2?

| Loss | Purpose |
|------|---------|
| **InfoNCE** | Discriminative: different tasks â†’ different bottlenecks |
| **L2** | Generative: exact bottleneck alignment |
| **Combined** | Fast convergence + task separation |

### 7.4 Why Graph Transformer for Language?

- **Matches teacher architecture**: Same attention mechanism
- **Spatial reasoning**: Positional edge encodings preserved
- **Language broadcast**: Single language node attends to all scene/gripper nodes
- **Permutation equivariant**: No fixed ordering of scene points

### 7.5 Why Dummy Demos at Inference?

**The Critical Insight**: At inference, we must run the full pipeline (`local_encoder` â†’ `cond_encoder` â†’ inject bottleneck â†’ `action_encoder`) to get correct actions. But `cond_encoder` requires demo nodes in the graph to function.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHY DUMMY DEMOS ARE NECESSARY                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  WRONG APPROACH (Skip cond_encoder):                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚                                                                             â”‚
â”‚    local_encoder â†’ [INJECT BOTTLENECK] â†’ action_encoder                     â”‚
â”‚                            â†‘                    â†“                           â”‚
â”‚                     Language bottleneck    BROKEN ACTIONS!                  â”‚
â”‚                                            (tiny movements)                 â”‚
â”‚                                                                             â”‚
â”‚    Problem: action_encoder was trained on cond_encoder output.              â”‚
â”‚             Skipping cond_encoder = wrong feature distribution.             â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  CORRECT APPROACH (Run cond_encoder with dummy demos):                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚                                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                      â”‚
â”‚    â”‚  DUMMY DEMOS    â”‚  â† All zeros, but correct graph structure            â”‚
â”‚    â”‚  (num_demos=2)  â”‚                                                      â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â”‚
â”‚             â”‚                                                               â”‚
â”‚             â–¼                                                               â”‚
â”‚    local_encoder â”€â”€â–º cond_encoder â”€â”€â–º [INJECT BOTTLENECK] â”€â”€â–º action_encoderâ”‚
â”‚                           â”‚                    â†‘                    â”‚       â”‚
â”‚                           â”‚             Language bottleneck         â”‚       â”‚
â”‚                           â”‚                                         â”‚       â”‚
â”‚                           â–¼                                         â–¼       â”‚
â”‚                   Updates SCENE nodes              CORRECT ACTIONS!         â”‚
â”‚                   (cross-attention)                (proper magnitude)       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Two reasons we need dummy demos:**

1. **Graph Topology**: The teacher's graph structure depends on `num_demos`:
   - Node count: `num_demos Ã— traj_horizon Ã— (scene_nodes + gripper_nodes)`
   - Edge count: Dense connections scale with node count
   - Positional encodings: Relative positions between all node pairs

   Changing `num_demos` at inference breaks these learned patterns.

2. **Scene Feature Processing**: The `cond_encoder` doesn't just update gripper nodesâ€”it also updates **scene nodes** via cross-attention edges:
   ```python
   cond_encoder edges:
     ('scene', 'rel_demo', 'gripper')  # Scene attends to demo grippers
     ('scene', 'rel_demo', 'scene')    # Scene attends to demo scenes
   ```

   The `action_encoder` expects these processed scene features. Without running `cond_encoder`, scene features are wrong â†’ actions are wrong.

**The Solution**:

```python
# At inference (eval_language.py)
num_demos = config['num_demos']  # Use checkpoint's num_demos (e.g., 2)

# Create dummy demos with ZERO content but CORRECT structure
dummy_demo = {
    'obs': [np.zeros((num_points, 3)) for _ in range(traj_horizon)],
    'T_w_es': [np.eye(4) for _ in range(traj_horizon)],
    'grips': [0.0 for _ in range(traj_horizon)]
}

# Build data with correct number of dummy demos
full_sample = {
    'demos': [dummy_demo for _ in range(num_demos)],  # â† KEY: num_demos copies
    'live': {...}
}

# Now the forward pass works correctly:
# 1. local_encoder processes all nodes (including dummy demo nodes)
# 2. cond_encoder runs (updates scene features, dummy demo info is zeros)
# 3. Language bottleneck OVERWRITES the gripper features
# 4. action_encoder sees correct feature distribution â†’ correct actions
```

**Key Point**: The dummy demos carry **zero information** (all zeros), but they maintain the **graph structure** that the model was trained on. The language bottleneck provides the actual task information by overwriting the gripper node features after `cond_encoder` runs.

### 7.6 Complete Edge Type Reference (All Components)

There are **multiple graphs** in this system, each with different edge types. Here's the complete breakdown:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EDGE TYPES ACROSS ALL COMPONENTS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  TEACHER MODEL (Original Instant Policy) - 3 separate encoders       â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”‚  1. LOCAL ENCODER Ïƒ (2 layers)                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚     Node types: ['scene', 'gripper']                                        â”‚
â”‚     Edge types (3):                                                         â”‚
â”‚       â€¢ ('scene', 'rel', 'scene')      - Scene nodes attend to each other   â”‚
â”‚       â€¢ ('scene', 'rel', 'gripper')    - Scene nodes attend to gripper      â”‚
â”‚       â€¢ ('gripper', 'rel', 'gripper')  - Gripper nodes attend to each other â”‚
â”‚                                                                             â”‚
â”‚     Purpose: Extract local spatial features from point cloud + gripper      â”‚
â”‚                                                                             â”‚
â”‚  2. CONDITIONAL ENCODER Ï† (2 layers)                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚     Node types: ['gripper', 'scene']                                        â”‚
â”‚     Edge types (4):                                                         â”‚
â”‚       â€¢ ('gripper', 'cond', 'gripper')     - Current gripper â† demo gripper â”‚
â”‚       â€¢ ('gripper', 'demo', 'gripper')     - Demo gripper â† demo gripper    â”‚
â”‚       â€¢ ('scene', 'rel_demo', 'gripper')   - Scene â† demo gripper           â”‚
â”‚       â€¢ ('scene', 'rel_demo', 'scene')     - Scene â† demo scene             â”‚
â”‚                                                                             â”‚
â”‚     Purpose: Aggregate demonstration info into current observation          â”‚
â”‚     Output: The BOTTLENECK (current gripper nodes after demo aggregation)   â”‚
â”‚                                                                             â”‚
â”‚  3. ACTION ENCODER Ïˆ (2 layers)                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚     Node types: ['gripper', 'scene']                                        â”‚
â”‚     Edge types (4):                                                         â”‚
â”‚       â€¢ ('gripper', 'time_action', 'gripper') - Future gripper â† current    â”‚
â”‚       â€¢ ('gripper', 'rel_cond', 'gripper')    - Future â† conditioned gripperâ”‚
â”‚       â€¢ ('scene', 'rel_action', 'gripper')    - Scene â† action gripper      â”‚
â”‚       â€¢ ('scene', 'rel_action', 'scene')      - Scene â† scene               â”‚
â”‚                                                                             â”‚
â”‚     Purpose: Generate action predictions from bottleneck                    â”‚
â”‚                                                                             â”‚
â”‚  TEACHER MODEL TOTAL: 11 edge types (3 + 4 + 4)                             â”‚
â”‚                                                                             â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  LANGUAGE ENCODER Î¸ (New, for language modality transfer)            â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                             â”‚
â”‚  LANGUAGE ENCODER Î¸ (4 layers)                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚     Node types: ['scene', 'gripper', 'language']  â† NEW: language node!     â”‚
â”‚     Edge types (6):                                                         â”‚
â”‚       â€¢ ('scene', 'rel', 'scene')           - Scene â† scene (256 edges)     â”‚
â”‚       â€¢ ('scene', 'rel', 'gripper')         - Scene â† gripper (96 edges)    â”‚
â”‚       â€¢ ('gripper', 'rel', 'gripper')       - Gripper â† gripper (36 edges)  â”‚
â”‚       â€¢ ('language', 'lang_to_scene', 'scene')    - Lang â†’ scene (16 edges) â”‚
â”‚       â€¢ ('language', 'lang_to_gripper', 'gripper') - Lang â†’ gripper (6 edges)â”‚
â”‚       â€¢ ('language', 'lang_self', 'language')     - Self-loop (1 edge)      â”‚
â”‚                                                                             â”‚
â”‚     Purpose: Fuse language embedding with current observation               â”‚
â”‚     Output: Language bottleneck (replaces demo bottleneck at inference)     â”‚
â”‚                                                                             â”‚
â”‚  LANGUAGE ENCODER TOTAL: 6 edge types                                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Observations:**

| Component | Edge Types | Node Types | Purpose |
|-----------|------------|------------|---------|
| `local_encoder` | 3 | 2 (scene, gripper) | Extract spatial features |
| `cond_encoder` | 4 | 2 (scene, gripper) | Aggregate demo info â†’ bottleneck |
| `action_encoder` | 4 | 2 (scene, gripper) | Generate actions from bottleneck |
| `lang_encoder` | 6 | 3 (scene, gripper, **language**) | Language â†’ bottleneck |

**Why Language Encoder Has 6 Edge Types:**

The first 3 edge types (`rel` edges) are **similar** to the teacher's `local_encoder`:
- `('scene', 'rel', 'scene')` - Same as local_encoder
- `('scene', 'rel', 'gripper')` - Same as local_encoder
- `('gripper', 'rel', 'gripper')` - Same as local_encoder

The last 3 edge types are **NEW** for language:
- `('language', 'lang_to_scene', 'scene')` - Language broadcasts to all scene nodes
- `('language', 'lang_to_gripper', 'gripper')` - Language broadcasts to all gripper nodes
- `('language', 'lang_self', 'language')` - Required self-loop for PyG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LANGUAGE ENCODER EDGE CONNECTIVITY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                              â”‚  LANGUAGE   â”‚                                â”‚
â”‚                              â”‚   (1 node)  â”‚                                â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                     â”‚                                       â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚                â”‚                â”‚                      â”‚
â”‚              lang_to_scene    lang_self       lang_to_gripper               â”‚
â”‚              (16 edges)       (1 edge)        (6 edges)                     â”‚
â”‚                    â”‚                â”‚                â”‚                      â”‚
â”‚                    â–¼                â–¼                â–¼                      â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚      SCENE       â”‚              â”‚     GRIPPER      â”‚              â”‚
â”‚         â”‚   (16 nodes)     â”‚â—„â”€â”€â”€â”€relâ”€â”€â”€â”€â”€â–ºâ”‚    (6 nodes)     â”‚              â”‚
â”‚         â”‚                  â”‚   (96 edges) â”‚                  â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                  â”‚                                 â”‚                        â”‚
â”‚                  â””â”€â”€â”€â”€â”€relâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€relâ”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                   (256 edges)  â”‚    â”‚  (36 edges)                           â”‚
â”‚                                â–¼    â–¼                                       â”‚
â”‚                              self-loops                                     â”‚
â”‚                                                                             â”‚
â”‚  TOTAL: 6 edge types, 411 edges per sample                                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Are Any Edge Types "The Same"?**

| Edge Type | Language Encoder | Teacher's local_encoder | Same Name? | Same Weights? |
|-----------|------------------|------------------------|------------|---------------|
| `('scene', 'rel', 'scene')` | âœ… | âœ… | Yes | **No** - separate models |
| `('scene', 'rel', 'gripper')` | âœ… | âœ… | Yes | **No** - separate models |
| `('gripper', 'rel', 'gripper')` | âœ… | âœ… | Yes | **No** - separate models |
| `('language', 'lang_to_*')` | âœ… | âŒ | N/A | N/A - only in lang_encoder |

**Key Point**: Even though the names are the same, the **weights are completely separate**. The language encoder is a new model trained from scratch with its own parameters.

---

### 7.7 What Happens to Scene Nodes?

**The Critical Question**: If the language bottleneck only overwrites **gripper nodes**, what happens to the **scene nodes**?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SCENE NODE PROCESSING DURING INFERENCE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  STAGE 1: local_encoder (â„ï¸ frozen)                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                               â”‚
â”‚                                                                             â”‚
â”‚    Current Observation                 Dummy Demos                          â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚    Point cloud (16 nodes)              Zero point clouds Ã— 2 demos          â”‚
â”‚    Gripper pose (6 nodes)              Zero gripper poses Ã— 2 demos         â”‚
â”‚           â”‚                                    â”‚                            â”‚
â”‚           â–¼                                    â–¼                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚              local_encoder Ïƒ (Graph Transformer)            â”‚          â”‚
â”‚    â”‚    Processes spatial relationships, extracts features       â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                                    â”‚                            â”‚
â”‚           â–¼                                    â–¼                            â”‚
â”‚    current_scene_x: [B, 16, 1024]       demo_scene_x: [B, 2Ã—10Ã—16, 1024]    â”‚
â”‚    current_gripper_x: [B, 6, 1024]      demo_gripper_x: [B, 2Ã—10Ã—6, 1024]   â”‚
â”‚                                                                             â”‚
â”‚    â˜… Scene nodes now have LOCAL features (spatial layout)                   â”‚
â”‚    â˜… But NO task context yet (don't know what to do)                        â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  STAGE 2: cond_encoder (â„ï¸ frozen)                                          â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                            â”‚
â”‚                                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚             cond_encoder Ï† (Graph Transformer)              â”‚          â”‚
â”‚    â”‚                                                             â”‚          â”‚
â”‚    â”‚  This encoder has cross-attention edges that UPDATE BOTH:   â”‚          â”‚
â”‚    â”‚                                                             â”‚          â”‚
â”‚    â”‚  â€¢ Gripper nodes: aggregate info from demo trajectories     â”‚          â”‚
â”‚    â”‚  â€¢ Scene nodes: cross-attend to demo scenes/grippers        â”‚          â”‚
â”‚    â”‚                                                             â”‚          â”‚
â”‚    â”‚  Edge types in cond_encoder:                                â”‚          â”‚
â”‚    â”‚    ('scene', 'rel_demo', 'scene')    Scene â† Demo scenes    â”‚          â”‚
â”‚    â”‚    ('scene', 'rel_demo', 'gripper')  Scene â† Demo grippers  â”‚          â”‚
â”‚    â”‚    ('gripper', 'rel_demo', 'scene')  Gripper â† Demo scenes  â”‚          â”‚
â”‚    â”‚    ('gripper', 'rel_demo', 'gripper') Gripper â† Demo grippersâ”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                                    â”‚                            â”‚
â”‚           â–¼                                    â–¼                            â”‚
â”‚    updated_scene_x: [B, 16, 1024]        (irrelevant: 0 â† 0)               â”‚
â”‚    updated_gripper_x: [B, 6, 1024]       (will be overwritten)             â”‚
â”‚                                                                             â”‚
â”‚    â˜… Scene nodes now have CONTEXTUAL features (cross-attended)              â”‚
â”‚    â˜… With dummy demos (zeros), scene nodes essentially just pass through    â”‚
â”‚      with minor transformations from the cond_encoder layers                â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  STAGE 3: Bottleneck Injection                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                              â”‚
â”‚                                                                             â”‚
â”‚    BEFORE INJECTION:                                                        â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚    scene_x:     [B, 16, 1024]  â† From cond_encoder (context-aware)          â”‚
â”‚    gripper_x:   [B, 6, 1024]   â† From cond_encoder (demo-informed, but 0)   â”‚
â”‚                                                                             â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                        â”‚   LANGUAGE BOTTLENECK  â”‚                           â”‚
â”‚                        â”‚   [B, 6, 1024]         â”‚                           â”‚
â”‚                        â”‚   (from lang_encoder)  â”‚                           â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼ OVERWRITE                              â”‚
â”‚    AFTER INJECTION:                                                         â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                         â”‚
â”‚    scene_x:     [B, 16, 1024]  â† UNCHANGED (keeps cond_encoder output)      â”‚
â”‚    gripper_x:   [B, 6, 1024]   â† REPLACED with language bottleneck          â”‚
â”‚                                                                             â”‚
â”‚    â˜… Scene nodes: Retain their processed features                           â”‚
â”‚    â˜… Gripper nodes: Completely replaced with language-derived features      â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  STAGE 4: action_encoder (â„ï¸ frozen)                                        â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                          â”‚
â”‚                                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚             action_encoder Ïˆ (Graph Transformer)            â”‚          â”‚
â”‚    â”‚                                                             â”‚          â”‚
â”‚    â”‚  Receives:                                                  â”‚          â”‚
â”‚    â”‚  â€¢ scene_x: [B, 16, 1024]   â† Local + contextual features   â”‚          â”‚
â”‚    â”‚  â€¢ gripper_x: [B, 6, 1024]  â† LANGUAGE bottleneck           â”‚          â”‚
â”‚    â”‚                                                             â”‚          â”‚
â”‚    â”‚  Cross-attention between:                                   â”‚          â”‚
â”‚    â”‚  â€¢ Gripper nodes (task intent from language)                â”‚          â”‚
â”‚    â”‚  â€¢ Scene nodes (spatial layout of environment)              â”‚          â”‚
â”‚    â”‚                                                             â”‚          â”‚
â”‚    â”‚  â†’ Generates action predictions based on WHERE (scene)      â”‚          â”‚
â”‚    â”‚    and WHAT (gripper/language bottleneck)                   â”‚          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚                          action_gripper_x: [B, 6, 1024]                     â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚                          prediction_heads â†’ ACTIONS                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Scene Nodes Don't Need to Be Overwritten:**

| Node Type | Information Source | Role in Action Generation |
|-----------|-------------------|---------------------------|
| **Scene** (16) | Current observation point cloud | WHERE: Spatial layout, object positions |
| **Gripper** (6) | Language bottleneck | WHAT: Task intent, target object, action type |

The key insight is that **scene nodes provide spatial context**, while **gripper nodes provide task intent**:

1. **Scene nodes answer WHERE**:
   - "Where are the objects?"
   - "What's the spatial layout?"
   - "Where is the button/phone/box?"

2. **Gripper nodes answer WHAT**:
   - "What task should I do?"
   - "Which object should I interact with?"
   - "What motion pattern should I follow?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INFORMATION FLOW SUMMARY                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  TRAINING (with real demos):                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚                                                                             â”‚
â”‚    Real Demos â”€â”€â–º cond_encoder â”€â”€â–º scene_x has DEMO-INFORMED context        â”‚
â”‚                                    gripper_x has DEMO-INFORMED task intent  â”‚
â”‚                                                                             â”‚
â”‚  INFERENCE (with language):                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚                                                                             â”‚
â”‚    Dummy Demos â”€â”€â–º cond_encoder â”€â”€â–º scene_x has MINIMAL context (near zero) â”‚
â”‚    Language â”€â”€â”€â”€â”€â–º lang_encoder â”€â”€â–º gripper_x has LANGUAGE-INFORMED intent  â”‚
â”‚                                    (overwrites the minimal gripper_x)       â”‚
â”‚                                                                             â”‚
â”‚  WHY THIS WORKS:                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚                                                                             â”‚
â”‚    The action_encoder Ïˆ was trained to:                                     â”‚
â”‚    â€¢ Extract spatial info from scene_x (positions, layout)                  â”‚
â”‚    â€¢ Extract task intent from gripper_x (what to do)                        â”‚
â”‚    â€¢ Cross-attend between them to generate actions                          â”‚
â”‚                                                                             â”‚
â”‚    At inference:                                                            â”‚
â”‚    â€¢ scene_x still has valid spatial info (from local_encoder)              â”‚
â”‚    â€¢ gripper_x now has language-derived task intent (from bottleneck)       â”‚
â”‚    â€¢ action_encoder combines them â†’ correct actions                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The Role of Dummy Demos for Scene Processing:**

Even though dummy demos are zeros, running `cond_encoder` is still necessary because:

1. **Feature Distribution**: The `action_encoder` was trained on features that passed through `cond_encoder`. Even with zero inputs, the cond_encoder applies transformations (bias terms, layer norm) that keep features in the expected distribution.

2. **Architecture Compatibility**: The graph structure must match what the model was trained on. Skipping `cond_encoder` entirely would change the feature flow.

3. **Scene Self-Attention**: Within `cond_encoder`, scene nodes still attend to each other (`scene â†’ scene` edges), refining their spatial representations even when demo information is zero.

**Practical Implication:**

```python
# What happens to each tensor through the pipeline:

# local_encoder output:
scene_x_local = [B, 16, 1024]     # Spatial features from point cloud
gripper_x_local = [B, 6, 1024]   # Gripper pose features

# cond_encoder output (with dummy demos):
scene_x_cond = [B, 16, 1024]     # Slightly transformed (bias, norm)
gripper_x_cond = [B, 6, 1024]    # Slightly transformed (will be discarded)

# After bottleneck injection:
scene_x_final = scene_x_cond     # UNCHANGED from cond_encoder
gripper_x_final = lang_bottleneck  # COMPLETELY REPLACED

# action_encoder input:
# - scene_x_final: Valid spatial context
# - gripper_x_final: Language-derived task intent
```

**Summary:**

| Question | Answer |
|----------|--------|
| Are scene nodes overwritten? | **No** - they keep their `cond_encoder` output |
| Do scene nodes have valid info? | **Yes** - spatial layout from current observation |
| Why not overwrite scene nodes too? | Scene nodes provide WHERE, gripper nodes provide WHAT |
| What does the language bottleneck encode? | Task intent (WHAT to do), not spatial info (WHERE) |
| Why run cond_encoder at all? | Maintains feature distribution expected by action_encoder |

---

## 8. File Reference

### Core Implementation

| File | Purpose | Lines |
|------|---------|-------|
| `models/language_encoder.py` | Language-conditioned graph transformer | 148 |
| `models/model.py` | AGI model with bottleneck methods | +50 lines added |
| `train_language.py` | Training loop | 229 |
| `eval_language.py` | Inference & evaluation | 327 |
| `configs/language_config.py` | Hyperparameters | 13 |

### Utilities

| File | Purpose |
|------|---------|
| `utils/language_utils.py` | Sentence-BERT encoding, task templates |
| `scripts/build_language_dataset.py` | Add language annotations to data |
| `utils/running_dataset.py` | Dataset class with `require_lang` flag |

### Added Methods to `model.py`

| Method | Purpose |
|--------|---------|
| `get_demo_bottleneck(data)` | Extract teacher's bottleneck for supervision |
| `forward_from_bottleneck(data, bottleneck)` | Inject bottleneck and run action decoder |
| `_get_current_gripper_mask()` | Identify current timestep gripper nodes |
| `_ensure_scene_embeddings(data)` | Lazy compute scene embeddings |
| `_ensure_diff_time(data)` | Ensure diffusion timestep exists |

---

## 9. Usage Guide

### 9.1 Data Preparation

```bash
# Add language annotations to training data
python scripts/build_language_dataset.py \
    --data_dir ./data/train/push_button \
    --task_name push_button \
    --device cuda \
    --add_text
```

### 9.2 Training

```bash
python train_language.py \
    --model_path ./checkpoints \
    --data_path_train ./data/train/push_button \
    --batch_size 16 \
    --max_steps 100000 \
    --save_dir ./runs_lang \
    --device cuda \
    --use_wandb 1
```

### 9.3 Evaluation

```bash
# Single instruction
python eval_language.py \
    --task_name push_button \
    --lang_encoder_path ./runs_lang/lang_encoder_50000.pt \
    --num_rollouts 10

# Custom text
python eval_language.py \
    --task_name push_button \
    --lang_text "Hit the red button firmly." \
    --lang_encoder_path ./runs_lang/lang_encoder_50000.pt

# Paraphrase robustness
python eval_language.py \
    --task_name push_button \
    --paraphrase_file paraphrases.txt \
    --lang_encoder_path ./runs_lang/lang_encoder_50000.pt
```

---

## 10. Results & Evaluation

### 10.1 Expected Performance

| Metric | Demo-Based | Language-Based |
|--------|------------|----------------|
| Success Rate | 80-95% | 50-80% |
| Paraphrase SR | N/A | 40-70% |
| Inference Speed | ~50ms/step | ~50ms/step |

### 10.2 Training Convergence

**Training run: 50,000 steps (Actual Results)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRAINING CURVES (ACTUAL)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  SIMILARITY (sim)                    LOSS                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€                                  â”‚
â”‚  1.0 â”¤                         â—â—â—   4.0 â”¤â—                                â”‚
â”‚      â”‚            â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—       â”‚                                 â”‚
â”‚  0.8 â”¤      â—â—â—â—â—â—                   3.0 â”¤                                 â”‚
â”‚      â”‚    â—â—                             â”‚                                 â”‚
â”‚  0.6 â”¤   â—                           2.0 â”¤                                 â”‚
â”‚      â”‚  â—                                â”‚                                 â”‚
â”‚  0.4 â”¤                               1.0 â”¤                                 â”‚
â”‚      â”‚ â—                                 â”‚    â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—  â”‚
â”‚  0.2 â”¤â—                              0.0 â”¼â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â–º   â”‚
â”‚      â”¼â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â–º          0   10k  20k  30k  40k  50k       â”‚
â”‚      0   10k  20k  30k  40k  50k                                           â”‚
â”‚                                                                            â”‚
â”‚  L2 LOSS                             CONTRASTIVE LOSS                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  12 â”¤â—                               2.5 â”¤â—                                â”‚
â”‚     â”‚                                    â”‚                                 â”‚
â”‚  10 â”¤ â—                              2.0 â”¤                                 â”‚
â”‚     â”‚  â—                                 â”‚                                 â”‚
â”‚   8 â”¤   â—                            1.5 â”¤                                 â”‚
â”‚     â”‚    â—                               â”‚                                 â”‚
â”‚   4 â”¤     â—â—                         1.0 â”¤                                 â”‚
â”‚     â”‚       â—â—â—â—                         â”‚                                 â”‚
â”‚   2 â”¤          â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—    0.5 â”¤  â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—   â”‚
â”‚     â”¼â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â–º       0.0 â”¼â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â–º        â”‚
â”‚     0   10k  20k  30k  40k  50k          0   10k  20k  30k  40k  50k       â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Final Metrics at Step 49,800:**

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Similarity** | **0.952** | Excellent bottleneck alignment |
| **Total Loss** | **0.100** | Well converged |
| **L2 Loss** | **0.188** | Direct bottleneck matching |
| **Contrastive** | **0.081** | Strong task discrimination |

**Convergence Summary:**

| Step | Similarity | Loss | L2 Loss | Contrastive |
|------|------------|------|---------|-------------|
| 0 | ~0.0 | ~4.0 | ~12.0 | ~2.5 |
| 5k | ~0.80 | ~0.2 | ~2.0 | ~0.2 |
| 10k | ~0.90 | ~0.15 | ~0.5 | ~0.1 |
| **50k** | **0.952** | **0.100** | **0.188** | **0.081** |

**Key Observations:**
- **Rapid early convergence**: Similarity jumps from 0 â†’ 0.8 in first 5k steps
- **Stable plateau**: Metrics stabilize after ~10k steps
- **High final similarity (0.95)**: Language bottleneck nearly matches demo bottleneck
- **Low contrastive loss (0.08)**: Strong discrimination between different tasks

### 10.3 Supported Tasks (17 RLBench Tasks)

```
push_button, phone_on_base, slide_block, close_box,
lift_lid, open_box, basketball, buzz, close_microwave,
plate_out, toilet_seat_down, toilet_seat_up, toilet_roll_off,
open_microwave, lamp_on, umbrella_out, put_rubbish
```

---

## Summary

### What We Implemented

1. **Language Encoder** (`LanguageConditionedEncoder`): 4-layer graph transformer that fuses current observation with language semantics to produce a 6Ã—1024D bottleneck

2. **Bottleneck Injection**: Modified `AGI.forward_from_bottleneck()` to inject language bottleneck into the frozen action decoder

3. **Training Pipeline**: Contrastive + L2 loss to align language bottlenecks with demo bottlenecks

4. **Inference Pipeline**: Language â†’ SBERT â†’ bottleneck â†’ frozen decoder â†’ actions

5. **Evaluation**: Success rate and paraphrase robustness testing

### Key Innovations

- **Non-invasive**: Only ~50 lines added to teacher model
- **Modular**: Language encoder is separate, swappable
- **Efficient**: Bottleneck computed once per control step
- **Zero-shot**: No task-specific fine-tuning required

### Theoretical Contribution

This implementation demonstrates **modality transfer via learned bottlenecks**:
- Same action decoder works for demos AND language
- Bottleneck serves as universal task representation
- Extensible to other modalities (images, sketches, haptics)

---

## References

- **Paper**: Instant Policy: In-Context Imitation Learning via Graph Diffusion (Appendix J)
- **Sentence-BERT**: Reimers & Gurevych, 2019
- **InfoNCE**: Oord et al., 2018
- **RLBench**: James et al., 2020

---

*This implementation enables robots to understand and execute tasks from natural language, bridging the gap between human communication and robot action.*
